{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bff8c256",
   "metadata": {},
   "source": [
    "# Training Neural Networks\n",
    "In this exercise, you will train a neural network using PyTorch. You will be provided some starter code and will fill in the blanks. \n",
    "\n",
    "This will walk through the entire process, from loading datasets to creating transforms, all the way through to creating the network code and training it to classify the CIFAR-10 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2c8598f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT EDIT THIS CELL\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision import datasets\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1dba3465",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9700ad26",
   "metadata": {},
   "source": [
    "## Loading and Preprocessing Data\n",
    "In this section, we will load and preprocess our data using any relevant methods from `transforms` and `datasets`.\n",
    "Then, we will create `DataLoader`s for our train and test sets.\n",
    "\n",
    "If you have trouble, feel free to consult the documentation for [transforms](https://pytorch.org/vision/0.12/transforms.html) and [CIFAR-10](https://pytorch.org/vision/stable/generated/torchvision.datasets.CIFAR10.html#torchvision.datasets.CIFAR10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ffcd6c5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Establish our transform\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "# Load train and test datasets\n",
    "training_data = datasets.CIFAR10(root=\"data\", train=True, download=True, transform=transform)\n",
    "test_data = datasets.CIFAR10(root=\"data\", train=False, download=True, transform=transform)\n",
    "\n",
    "# Create the training and test dataloaders with a batch size of 32\n",
    "train_loader = DataLoader(training_data, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccab776a",
   "metadata": {},
   "source": [
    "## Defining our Neural Network\n",
    "Once our data is loaded, we want to define our model. \n",
    "For this example, we want to use a fully-connected model, which means we will need to use the `flatten` method to take our 32 x 32 x 3 tensor and flatten it into a single input. \n",
    "\n",
    "We want to have at least 3 fully connected layers. \n",
    "The input size of the first layer will need to account for the flattening and will be 32 * 32 * 3.\n",
    "Feel free to experiment here, and if you need additional help, consult the [PyTorch documentation](https://pytorch.org/docs/stable/nn.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0527ea2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (fc1): Linear(in_features=3072, out_features=120, bias=True)\n",
       "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
       "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the class for your neural network\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.activation = F.relu\n",
    "        self.fc1 = nn.Linear(32 * 32 * 3, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = self.activation(self.fc1(x))\n",
    "        x = self.activation(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Instantiate the model\n",
    "net = Net()\n",
    "net.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188857b1",
   "metadata": {},
   "source": [
    "## Optimizer and Loss function\n",
    "Before we get into our training loop, we need to choose an optimizer and loss function for our network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "825c4c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose an optimizer\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "\n",
    "# Choose a loss function\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "685d94b8",
   "metadata": {},
   "source": [
    "## Creating the Training Loop\n",
    "With our network, optimizer, and loss function, we can now begin the training step! \n",
    "Using the test set to validate our accuracy, we can see when our network has given us the best fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd12452",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 training accuracy: 41.71% training loss: 1.64691\n",
      "Epoch 1 validation accuracy: 46.12% validation loss: 1.51591\n",
      "Epoch 2 training accuracy: 48.56% training loss: 1.45633\n",
      "Epoch 2 validation accuracy: 48.82% validation loss: 1.45180\n",
      "Epoch 3 training accuracy: 51.93% training loss: 1.36100\n",
      "Epoch 3 validation accuracy: 50.36% validation loss: 1.40834\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "\n",
    "# Establish a list for our history\n",
    "train_loss_history = list()\n",
    "val_loss_history = list()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    net.train()\n",
    "    train_loss = 0.0\n",
    "    train_correct = 0\n",
    "    for i, data in enumerate(train_loader):\n",
    "        # data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "\n",
    "        # Pass to GPU if available.\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        _, preds = torch.max(outputs.data, 1)\n",
    "        train_correct += (preds == labels).float().mean().item()\n",
    "        train_loss += loss.item()\n",
    "    print(f'Epoch {epoch + 1} training accuracy: {train_correct/len(train_loader)*100:.2f}% training loss: {train_loss/len(train_loader):.5f}')\n",
    "    train_loss_history.append(train_loss/len(train_loader))\n",
    "\n",
    "\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    net.eval()\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        _, preds = torch.max(outputs.data, 1)\n",
    "        val_correct += (preds == labels).float().mean().item()\n",
    "        val_loss += loss.item()\n",
    "    print(f'Epoch {epoch + 1} validation accuracy: {val_correct/len(test_loader)*100:.2f}% validation loss: {val_loss/len(test_loader):.5f}')\n",
    "    val_loss_history.append(val_loss/len(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf406b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training and validation loss history\n",
    "plt.plot(train_loss_history, label=\"Training Loss\")\n",
    "plt.plot(val_loss_history, label=\"Validation Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3fb0f53",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
