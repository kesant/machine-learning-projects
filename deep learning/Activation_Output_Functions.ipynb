{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56502e78",
   "metadata": {},
   "source": [
    "# Activation and Output Functions\n",
    "In this exercise, you'll explore how activation functions and output functions impact the ability of neural networks to learn. \n",
    "\n",
    "Most of the code will be provided for you, and you'll have to fill in the blanks! \n",
    "Consider trying a few different combinations of activation functions to get a better idea of how the activation function impacts training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ff0b0a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT EDIT THIS CELL\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d127592b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a5d842",
   "metadata": {},
   "source": [
    "## Loading Data\n",
    "We use the [CIFAR-10](https://pytorch.org/vision/stable/generated/torchvision.datasets.CIFAR10.html#torchvision.datasets.CIFAR10) dataset from the `torchvision` module and wrap the training and test datasets in a DataLoader. \n",
    "\n",
    "We also create a `train_network` function that takes a PyTorch neural network, a train DataLoader, and a test DataLoader.\n",
    "\n",
    "This code has been provided for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b60bb6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Establish our transform\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "# Load train and test datasets\n",
    "training_data = datasets.CIFAR10(root=\"data\", train=True, download=True, transform=transform)\n",
    "test_data = datasets.CIFAR10(root=\"data\", train=False, download=True, transform=transform)\n",
    "\n",
    "# Create the training and test dataloaders with a batch size of 32\n",
    "train_loader = DataLoader(training_data, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f779157b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code is provided for you. \n",
    "# Feel free to look it over but if you modify it, it may break!\n",
    "def train_network_classification(net, train_loader, test_loader):\n",
    "    num_epochs = 10\n",
    "    \n",
    "    # We'll use Negative Log Likelihood Loss as our objective function here. Leave it fixed for now.\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    # Don't worry about the choice of optimizer here. Leave it fixed for now.\n",
    "    optimizer = optim.SGD(mlp.parameters(), lr=0.005, momentum=0.9)\n",
    "    \n",
    "    # Establish a list for our history\n",
    "    train_loss_history = list()\n",
    "    val_loss_history = list()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        net.train()\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        for i, data in enumerate(train_loader):\n",
    "            # data is a list of [inputs, labels]\n",
    "            inputs, labels = data\n",
    "            \n",
    "            # Pass to GPU if available.\n",
    "            if torch.cuda.is_available():\n",
    "                inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            _, preds = torch.max(outputs.data, 1)\n",
    "            train_correct += (preds == labels).sum().item()\n",
    "            train_loss += loss.item()\n",
    "        print(f'Epoch {epoch + 1} training accuracy: {train_correct/len(train_loader):.2f}% training loss: {train_loss/len(train_loader):.5f}')\n",
    "        train_loss_history.append(train_loss)\n",
    "\n",
    "        \n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        net.eval()\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            _, preds = torch.max(outputs.data, 1)\n",
    "            val_correct += (preds == labels).sum().item()\n",
    "            val_loss += loss.item()\n",
    "        print(f'Epoch {epoch + 1} validation accuracy: {val_correct/len(test_loader):.2f}% validation loss: {val_loss/len(test_loader):.5f}')\n",
    "        val_loss_history.append(val_loss)           \n",
    "\n",
    "    plt.plot(train_loss_history, label=\"Training Loss\")\n",
    "    plt.plot(val_loss_history, label=\"Validation Loss\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "def train_network_regression(net, train_loader, test_loader):\n",
    "    num_epochs = 10\n",
    "    \n",
    "    criterion = nn.L1Loss(reduction='sum')\n",
    "\n",
    "    # Don't worry about the choice of optimizer here. Leave it fixed for now.\n",
    "    optimizer = optim.SGD(mlp.parameters(), lr=0.05)\n",
    "    \n",
    "    # Establish a list for our history\n",
    "    train_loss_history = list()\n",
    "    val_loss_history = list()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        net.train()\n",
    "        train_loss = 0.0\n",
    "        for i, data in enumerate(train_loader):\n",
    "            # data is a list of [inputs, labels]\n",
    "            inputs, labels = data\n",
    "            \n",
    "            # Pass to GPU if available.\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "        print(f'Epoch {epoch + 1} training loss: {train_loss/len(train_loader):.5f}')\n",
    "        train_loss_history.append(train_loss)\n",
    "\n",
    "        \n",
    "        val_loss = 0.0\n",
    "        net.eval()\n",
    "        for inputs, labels in test_loader:\n",
    "            if torch.cuda.is_available():\n",
    "                inputs, labels = inputs.cuda(), labels.cuda()\n",
    "            \n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            val_loss += loss.item()\n",
    "        print(f'Epoch {epoch + 1} validation loss: {val_loss/len(test_loader):.5f}')\n",
    "        val_loss_history.append(val_loss)           \n",
    "\n",
    "    plt.plot(train_loss_history, label=\"Training Loss\")\n",
    "    plt.plot(val_loss_history, label=\"Validation Loss\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c497413",
   "metadata": {},
   "source": [
    "## Defining a Neural Network\n",
    "The first model we establish is a fully-connected neural network -- a multi-layer perceptron. \n",
    "You will specify the activation and output function for the network based on the task -- a 10-class image classification task.\n",
    "\n",
    "If you need to, consult the [PyTorch documentation](https://pytorch.org/docs/stable/nn.functional.html#non-linear-activation-functions) for the activation and output function options available to you.\n",
    "\n",
    "**NOTE:** When choosing your activation and output functions, omit the parentheses in the assignment to the class property."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "734fb18d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CIFAR_MLP(\n",
       "  (fc1): Linear(in_features=3072, out_features=120, bias=True)\n",
       "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
       "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class CIFAR_MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.activation = F.relu\n",
    "        self.output = F.log_softmax\n",
    "        self.fc1 = nn.Linear(32 * 32 * 3, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = self.activation(self.fc1(x))\n",
    "        x = self.activation(self.fc2(x))\n",
    "        x = self.output(self.fc3(x), dim=1)\n",
    "        return x\n",
    "\n",
    "# Do not change the name of your model or later cells may fail!\n",
    "mlp = CIFAR_MLP()\n",
    "mlp.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b736ea05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 training accuracy: 12.50% training loss: 1.70819\n",
      "Epoch 1 validation accuracy: 14.48% validation loss: 1.53703\n",
      "Epoch 2 training accuracy: 15.24% training loss: 1.47401\n",
      "Epoch 2 validation accuracy: 15.43% validation loss: 1.45282\n",
      "Epoch 3 training accuracy: 16.49% training loss: 1.37568\n",
      "Epoch 3 validation accuracy: 16.29% validation loss: 1.40309\n",
      "Epoch 4 training accuracy: 17.17% training loss: 1.30747\n",
      "Epoch 4 validation accuracy: 16.27% validation loss: 1.40993\n",
      "Epoch 5 training accuracy: 17.89% training loss: 1.25086\n",
      "Epoch 5 validation accuracy: 16.60% validation loss: 1.38988\n"
     ]
    }
   ],
   "source": [
    "# Now let's train our network!\n",
    "train_network_classification(mlp, train_loader, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a4668f",
   "metadata": {},
   "source": [
    "## Regression Tasks\n",
    "In a regession task, we'll need to think about something else -- our same model may not work! \n",
    "For this task, we'll use the [California Housing Dataset](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_california_housing.html).\n",
    "\n",
    "Again, if you get stuck on your choices of activation function, check out [the documentation](https://pytorch.org/docs/stable/nn.functional.html#non-linear-activation-functions).\n",
    "This network looks a bit different -- why do we not have an output function for this task?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a10897e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load our data and split it into train and test sets\n",
    "data, target = fetch_california_housing(return_X_y=True)\n",
    "train_x, test_x, train_y, test_y = train_test_split(data, target, test_size=0.3)\n",
    "\n",
    "# Since we are using PyTorch, we need tensors!\n",
    "train_x = torch.tensor(train_x, dtype=torch.float32)\n",
    "test_x = torch.tensor(test_x, dtype=torch.float32)\n",
    "train_y = torch.tensor(train_y, dtype=torch.float32)\n",
    "test_y = torch.tensor(test_y, dtype=torch.float32)\n",
    "\n",
    "# Then we convert those tensors to a TensorDataset\n",
    "train_california = torch.utils.data.TensorDataset(train_x, train_y)\n",
    "test_california = torch.utils.data.TensorDataset(test_x, test_y)\n",
    "\n",
    "# And create our DataLoaders!\n",
    "train_loader = DataLoader(train_california, batch_size=10, shuffle=True)\n",
    "test_loader = DataLoader(test_california, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3f1ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Housing_MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.activation = F.relu\n",
    "        self.hidden = nn.Linear(8, 2)\n",
    "        self.prediction = nn.Linear(2, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.activation(self.hidden(x))\n",
    "        x = self.prediction(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "# Do not change the name of your model or later cells may fail!\n",
    "mlp = Housing_MLP()\n",
    "mlp.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4c2f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's train our network!\n",
    "train_network_regression(mlp, train_loader, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d366ac1",
   "metadata": {},
   "source": [
    "Great work! Go back and try different activation and output functions throughout and see how it affects your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee9286f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
