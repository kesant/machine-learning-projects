{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb9f9572-6664-42e9-a160-f22dfeea0a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import KNN from the relevant pyod module\n",
    "from pyod.models.knn import KNN\n",
    "import pandas as np\n",
    "import numpy as np\n",
    "# Import the euclidean function from scipy\n",
    "from scipy.spatial.distance import euclidean\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import QuantileTransformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e768d3-8d10-44b7-a195-14f0394204f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import KNN from the relevant pyod module\n",
    "from pyod.models.knn import KNN\n",
    "\n",
    "# Instantiate KNN and fit to females\n",
    "knn = KNN(contamination=0.005, n_neighbors=20, n_jobs=-1)\n",
    "knn.fit(females)\n",
    "\n",
    "# Create a boolean index that checks for outliers\n",
    "is_outlier =knn.labels_==1\n",
    "\n",
    "# Isolate the outliers\n",
    "outliers = females[is_outlier]\n",
    "\n",
    "print(len(outliers))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c04afc95-9cd4-4cb7-a277-a37a8104634b",
   "metadata": {},
   "source": [
    "# KNN with outlier probabilities\r\n",
    "Since we cannot wholly trust the output when using contamination, let's double-check our work using outlier probabilities. They are more trustworthy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3882318d-f524-46e1-8a00-d9c5dd89e7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a KNN with 20 neighbors and fit to `females`\n",
    "knn = KNN(n_neighbors=20,n_jobs=-1)\n",
    "knn.fit(females)\n",
    "\n",
    "# Calculate probabilities\n",
    "probs = knn.predict_proba(females)\n",
    "\n",
    "# Create a boolean  of 55%\n",
    "is_outlier = probs[:, 1]>0.55\n",
    "\n",
    "# Use the boolean mask to filter the outliers\n",
    "outliers = females[is_outlier]\n",
    "\n",
    "print(len(outliers))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de501ef6-5008-4cbe-96e7-c3c802106dde",
   "metadata": {},
   "source": [
    "# Finding the euclidean distance manually\n",
    "Euclidean distance is the most popular distance metric in statistics. Its popularity mainly comes from the fact that it is intuitive to understand. It is the Pythagorean theorem applied in Cartesian coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5bf22b1b-bc5c-46f1-9dcb-860198835e50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160.63934760823702\n"
     ]
    }
   ],
   "source": [
    "M = np.array([14, 17, 18, 20, 14, 12, 19, 13, 17, 20])\n",
    "N = np.array([63, 74, 76, 72, 64, 75, 75, 61, 50, 53])\n",
    "\n",
    "# Subtract M from N and square the result\n",
    "squared_diffs = (N - M) ** 2\n",
    "\n",
    "# Calculate the sum of squared differences\n",
    "sum_diffs = np.sum(squared_diffs)\n",
    "\n",
    "# Find the square root\n",
    "dist_MN = np.sqrt(sum_diffs)\n",
    "\n",
    "print(dist_MN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72769fa9-8085-4da7-8780-0b44ec0a0182",
   "metadata": {},
   "source": [
    "# Finding the euclidean distance with SciPy\r\n",
    "Instead of writing multiple lines of code to calculate the euclidean distance, you can use SciPy. The library not only contains the euclidean function, but more than 40 other distance metricsâ€”all a single import statement away."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0371ada-8758-458b-8b77-95250db72575",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160.63934760823702\n"
     ]
    }
   ],
   "source": [
    "M = np.array([14, 17, 18, 20, 14, 12, 19, 13, 17, 20])\n",
    "N = np.array([63, 74, 76, 72, 64, 75, 75, 61, 50, 53])\n",
    "\n",
    "# Use the euclidean function on M and N\n",
    "dist_MN = euclidean(M,N)\n",
    "\n",
    "print(dist_MN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18b23c2-5738-481d-8a02-af5243cd1c32",
   "metadata": {},
   "source": [
    "# Practicing standardization\n",
    "It is dangerous to use KNN on unknown distributions blindly. Its performance suffers greatly when the feature distributions don't have the same scales. Unscaled features will skew distance calculations and thus return unrealistic anomaly scores.\n",
    "\n",
    "A common technique to counter this is using standardization, which involves removing the mean from a feature and dividing it by the standard deviation. This has the effect of making the feature have a mean of 0 and a variance of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cba4892f-2e85-4035-a23f-c12a4d69b4be",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'females' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m ss \u001b[38;5;241m=\u001b[39m StandardScaler()\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Extract feature and target arrays\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m X \u001b[38;5;241m=\u001b[39m females\u001b[38;5;241m.\u001b[39mdrop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweightkg\u001b[39m\u001b[38;5;124m\"\u001b[39m,axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m) \n\u001b[0;32m      6\u001b[0m y \u001b[38;5;241m=\u001b[39m females[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweightkg\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Fit/transform X\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'females' is not defined"
     ]
    }
   ],
   "source": [
    "# Initialize a StandardScaler\n",
    "ss = StandardScaler()\n",
    "\n",
    "# Extract feature and target arrays\n",
    "X = females.drop(\"weightkg\",axis=1) \n",
    "y = females[\"weightkg\"]\n",
    "\n",
    "# Fit/transform X\n",
    "X_transformed = ss.fit_transform(X)\n",
    "\n",
    "# Fit/transform X but preserve the column names\n",
    "X.loc[:,:] = ss.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44aaa22-db97-4e37-b0d0-8a6037a4fb78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a5568e65-e99c-4851-89fd-810313e3c046",
   "metadata": {},
   "source": [
    "# Testing QuantileTransformer\n",
    "Standardization is prone to the same pitfalls as z-scores. Both use mean and standardization in their calculations, which makes them highly sensitive to extreme values.\n",
    "\n",
    "To get around this problem, you should use QuantileTransformer which uses quantiles. Quantiles of a distribution stay the same regardless of the magnitude of outliers.\n",
    "\n",
    "You should use StandardScaler when the data is normally distributed (which can be checked with a histogram). For other distributions, QuantileTransformer is a better choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1322cb34-518a-44a8-9ae4-57a7deed0cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate an instance that casts to normal\n",
    "qt = QuantileTransformer(output_distribution=\"normal\")\n",
    "\n",
    "# Fit and transform the feature array\n",
    "X.loc[:,:] = qt.fit_transform(X)\n",
    "\n",
    "# Plot a histogram of palm length\n",
    "plt.hist(X[\"palmlength\"], color='red')\n",
    "\n",
    "plt.xlabel(\"Palm length\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69eedb42-dbee-4af0-81d3-a3e0050babbd",
   "metadata": {},
   "source": [
    "# Calculating manhattan distance manually\r\n",
    "While euclidean distance is very popular, it only scales well beyond two or three-dimensional data. In these cases, you can use manhattan distance as an alternative. It has the advantage of working exceptionally well with datasets with many categorical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2070bb9b-a941-4b21-870c-7c55592df232",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "499\n"
     ]
    }
   ],
   "source": [
    "M = np.array([14, 17, 18, 20, 14, 12, 19, 13, 17, 20])\n",
    "N = np.array([63, 74, 76, 72, 64, 75, 75, 61, 50, 53])\n",
    "\n",
    "# Subtract M from N and find the absolute value\n",
    "abs_diffs = np.abs(N-M)\n",
    "\n",
    "# Calculate the final manhattan distance\n",
    "manhattan_dist_MN = np.sum(abs_diffs)\n",
    "\n",
    "print(manhattan_dist_MN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68676f60-0f04-4998-9b43-0e80d8883453",
   "metadata": {},
   "source": [
    "# Tuning n_neighbors\r\n",
    "n_neighbors is the most crucial parameter of KNN. When you are unsure about the number of outliers in the dataset (which happens often), you can't use the rule of thumb that suggests using 20 neighbors when contamination is below 10%.\r\n",
    "\r\n",
    "For such cases, you'll have to tune n_neighbors. Practice the process on the transformed version of the females dataset from the last exercise. It has been loaded as females_transformed. KNN estimator, evaluate_outlier_classifier and evaluate_regressor functions are also loaded.\r\n",
    "\r\n",
    "Here are the function bodies as reminders:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8899bf8-2345-47e5-ac98-fac1da2f59ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_outlier_classifier(model, data, threshold=.75):\n",
    "    model.fit(data)\n",
    "\n",
    "    probs = model.predict_proba(data)\n",
    "    inliers = data[probs[:, 1] <= threshold]\n",
    "\n",
    "    return inliers\n",
    "\n",
    "def evaluate_regressor(inliers):\n",
    "    X, y = inliers.drop(\"weightkg\", axis=1), inliers[['weightkg']]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=10, train_size=0.8)\n",
    "\n",
    "    lr = LinearRegression()\n",
    "    lr.fit(X_train, y_train)\n",
    "\n",
    "    preds = lr.predict(X_test)\n",
    "    rmse = mean_squared_error(y_test, preds, squared=False)\n",
    "\n",
    "    return round(rmse, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3f4a47-9f65-458b-8cdc-976afd9e1e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of values for n_neigbors\n",
    "n_neighbors = [5, 10, 20]\n",
    "scores = dict()\n",
    "\n",
    "for k in n_neighbors:\n",
    "    # Instantiate KNN with the current k\n",
    "    knn = KNN(n_neighbors=k, n_jobs=-1)\n",
    "    \n",
    "    # Find the inliers with the current KNN\n",
    "    inliers = evaluate_outlier_classifier(knn, females_transformed, .50)\n",
    "    \n",
    "    # Calculate and store RMSE into scores\n",
    "    scores[k] = evaluate_regressor(inliers)\n",
    "    \n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7008db-e790-4f95-80b1-b26596865359",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_neighbors = [5, 20]\n",
    "methods = ['largest', 'mean', 'median']\n",
    "scores = dict()\n",
    "\n",
    "for k, m in product(n_neighbors,methods):\n",
    "    # Create a KNN instance\n",
    "    knn = KNN(n_neighbors=k, method=m, n_jobs=-1)\n",
    "    \n",
    "    # Find the inliers with the current KNN\n",
    "    inliers = evaluate_outlier_classifier(knn,females_transformed,.55)\n",
    "\n",
    "    # Calculate and store RMSE into scores\n",
    "    scores[(k, m)] = evaluate_regressor(inliers)\n",
    "    \n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9398b25e-5345-41a4-949a-2452bad02e8c",
   "metadata": {},
   "source": [
    "# LOF for the first time\n",
    "LOF differs from KNN only in the internal algorithm and the lack of the method parameter. Practice detecting outliers with it using contamination filtering on the scaled version of females dataset from previous exercises.\n",
    "\n",
    "The dataset has been loaded as females_transformed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f6f900-1f34-4423-81b0-9c21eaabc340",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import LOF from its relevant module\n",
    "from pyod.models.lof import LOF\n",
    "\n",
    "# Instantiate LOF and fit to females_transformed\n",
    "lof = LOF(contamination=0.003,n_neighbors=20,n_jobs=-1)\n",
    "lof.fit(females_transformed)\n",
    "\n",
    "# Create a boolean index that checks for outliers\n",
    "is_outlier = lof.labels_==1\n",
    "\n",
    "# Isolate the outliers\n",
    "outliers = females_transformed[is_outlier]\n",
    "\n",
    "print(len(outliers))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb0ed407-e15e-495c-a673-e1d51cfdc97b",
   "metadata": {},
   "source": [
    "# LOF with outlier probabilities\r\n",
    "As always, double-check that the chosen contamination level is trustworthy by filtering the outliers with a probability threshold. The syntax is the same as with KNN.\r\n",
    "\r\n",
    "LOF estimator has already been imported, and the females_transformed dataset is also available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1389e2-43d9-4965-9667-3545ffcacb7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate an LOF with 20 neighbors and fit to the data\n",
    "lof = LOF(n_neighbors=20)\n",
    "lof.fit(females_transformed)\n",
    "\n",
    "# Calculate probabilities\n",
    "probs = lof.predict_proba(females_transformed)\n",
    "\n",
    "# Create a boolean mask\n",
    "is_outlier = probs[:,1]>0.5\n",
    "\n",
    "# Use the boolean mask to filter the outliers\n",
    "outliers = females_transformed[is_outlier]\n",
    "\n",
    "print(len(outliers))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
